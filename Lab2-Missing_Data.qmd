---
title: "Lab 2 - Missing Data"
output: 
  tufte::tufte_html:
    css: 
    tufte_variant: "envisioned"
    highlight: github-dark
    fig_height: 10
    fig_width: 14
    toc: true
    toc_depth: 1
execute: 
  message: false
  warning: false
format: 
  html:
    code-fold: false
    code-overflow: wrap
engine: knitr
---

-   If you are fitting a model, display the model output in a neatly formatted table. (The `gt` `tidy` and `kable` functions can help!). `Modelsummary` also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)

- If you are creating a plot, use `ggplot` or `base`and make sure they are publication ready. That means there are clear labels for all axes, titles, etc.

- Commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages.

-  When you're done, we should be able to knit the final version of the QMD in your GitHub as a HTML.

```{r}
#| message: false
#| 
library(tidyverse)
library(skimr)
library(mice) # missing data analysis and visualization
library(ggmice)# missing data analysis and visualization
library(naniar) # missing data visualization
library(finalfit) # missing data visualization
library(modelsummary) # table output for models
```

# 1. Data

-   Load the `brandsma` data from the **mice** package. Execute `?brandsma` to get a sense of the variables and where the data come from. Then make a subset of the `brandsma` data that contains the four variables `pup`, `iqv`, `ses`, `lpr` and `lpo`. Name that subset `d`.

```{r}
#| fig-width: 8
#| fig-height: 12
#| 

```

-   Look at yo data (describe the data along with the completeness of the data ). Use a table or a figure.

```{r}

```

# 2. Missing Data Patterns

> Use the `ggmice` `plot_pattern()` function to do a missing data pattern analysis. Adjust the code from lecture to do a similar analysis with variables `iqv`, `ses`, `lpr` and `lpo`.

```{r}

```

> In a sentence or two, tell me how many cases showed the most popular pattern and describe to me what that pattern is.


> Which variable had the largest number of missing observations?

# 3. Missing data dummies

-   All variables in `d` have missing observations, with the exception of `pup`, the participant id number. In lecture and in the text, we learned that we can make dummy variables to indicate whether we have missingness in a given column. So let's do that:

> Make four dummy variables named `r_iqv` through `r_lpo`. \* Code `r_iqv` such that it is a zero when `is.na(iqv)` and a one when `iqv` has an observed value. In the same way, make `r_ses` be the missing data dummy for `ses`, and so on.

> Save these four dummy variables in the `d` data set as `my_md_missing`

```{r}

```

# 4. Missing Data Patterns `r` dummies

-   The `plot_pattern` function is very slick and easy to use. But you should also know how to use your data wrangling skills to perform such an analysis by hand. Follow along and I'll show you how.

> Subset the `d` data so it only contains the `pup` identifiers and the four `r` dummy variables and call it `my_md_pattern`

```{r}
#| 


```

- Within the `count()` function, simultaneously count up the rows by all four of your `r` dummy variables. Also, make sure the results are sorted by setting `sort = TRUE`. If you're confused by this, take a look at the third example here (https://dplyr.tidyverse.org/reference/count.html).  You'll just be extending to four variables.

> Save the results as `my_md_pattern` and show the results.

```{r}

```

::: callout-tip
*Tip*: If you are successful, you will have a tibble with 11 rows and five columns. The rightmost column will be named `n`. If you look above, the results of this exercise should match up closely with the `md.pattern()` from the previous section.
:::

# 5. Missing data plot

> Run the code I've provided, below. If you are successful, it will return a nice missing data pattern plot. I recommend spending some time going through that code to make sure you fully understand what it's doing. Feel free to make any adjustments to the figure ðŸ™‚

```{r}
#| eval: false
#| 

my_md_pattern %>% 
  mutate(pattern = str_c("pattern ", letters[1:11], ". (n = ", n, ")")) %>% 
  pivot_longer(cols = c(-n, -pattern)) %>% 
  mutate(observed = factor(value),
         pattern = fct_rev(pattern)) %>% 
  
  ggplot(aes(x = name, y = pattern)) +
  geom_tile(aes(fill = observed)) +
  geom_text(aes(label = observed, color = observed)) +
  scale_fill_viridis_d(option = "D", end = .7) +
  scale_color_manual(values = c("grey90", "black"), breaks = NULL) +
  scale_x_discrete(NULL, expand = c(0, 0), position = "top") +
  scale_y_discrete(NULL, expand = c(0, 0)) +
  ggtitle("Missing data patterns") +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks = element_blank())

```

# 6. Missing mechanisms

> Using what we learned in class, can you determine if the missingness observed in `d` is MAR or MCAR? How? Show me.

```{r}

```

> What can you conclude about missing data mechanisms?

# 7. Simple impute

> Okay, we're finally ready to impute. impute with the \`my_md_missing\` data set and save the results as `imp1`. If you get warning messages, worry but don't worry. We'll address those in the next section.

```{r}
    m=5
```

# 8. Inspect

-   It may not look like it, but we went along with a lot of default settings with that last bit of code and we made at least one major goof. Let's inspect the damage.

    > Use the `str()` function to get an overview of the contents in your `imp1` object.

```{r}

```

    > You may have noticed there's a lot of stuff in there. Let's take a more focused look by executing `imp1$method`.

```{r}

```

-  With the last two bullets in mind, notice how we used the `pup` variable in the imputation formulas. Since `pup` is just a participant id number, this is a really bad idea. We're injecting random noise into our data.

-   Also notice that we used all four `r` dummy variables in the imputation formulas and we even had imputation formulas for the dummy variables themselves. This is also a really bad idea. I know the definition of what a variable is is getting a little hazy, here. But hopefully you are building an intuition that the four `r` dummy variables are different in nature from the original variables they were based on.

::: callout-tip
**Tip**. You're not really doing or interpreting anything in this section. Just make sure you include your code so I know you followed along.
:::

# 9. Let's use `mice()` again, but with a few improvements:

-   In our data statement, just select the four variables we want to impute data for

```{r}


```

-   Add the setting `method = "pmm"`, which uses predictive mean matching. If you want you can set `method = "norm"` and it will use a Bayesian regression approach. There are lots of different methods (some machine learning ones as well). Look here for more methods: (https://stefvanbuuren.name/fimd/sec-modelform.html).

-  Also, to make the next exercise easier, set `m = 5`, which will result in 5 imputed data sets.

> Save the results as `imp2`

```{r}

```

# 10. Check the Imputation

> Since we've been working so hard to make those imputed data sets, we may as well take a look at them. If you're tricky, you can actually directly extract the imputed data sets from the `imp2` object by hand. But there's an easier way. The `mice::complete()` function will do that for you. If you execute `complete(imp2, action = "long")`, you will get all four imputed data sets stacked vertically on one another in the long data format. Do this and save the results as `d_imp`.

```{r}


```

-   You'll notice our `d_imp` has two new columns. The`.imp` column tells you which imputation number is it. The `.id` column tells you which row number the data is from.

-   With your `d_imp` data, make a scatter plot with `lpr` on the x-axis, `lpo` on the y-axis, and faceted by `.imp`. To deal with the overplotting, I recommend you adjust the `size` and `alpha` settings within `geom_point()`. Since you've been learning about how to beautify your **ggplot2** figures, feel free to pretty up the visualization in other ways. Make yourself proud.

# 11. More imputation EDA

> Since our sample size is so large, it might be difficult to see the differences among the four imputed data sets with a scatter plot. Descriptive statistics might be more helpful, here. Group the `d_imp` data by `.imp` and then use the `summarise()` function to compute:
>
> -   The mean of `lpo`,
>
> -   The standard deviation of `lpo`, and
>
> -   The correlation between `lpo` and `lpr`
>
> If you do this right, you'll get four similar--but slightly different--values for each of those descriptive statistics.

```{r}

```

# 12. Fit the model(s)

> Run `mice` again to fit a model with multiple imputed data sets (make sure to not include extraneous variables

```{r}

```

> Now follow the next step, corresponding to `with()` line. The regression model you are fitting is `lpo ~ lpr`. Save the results as `fit`.

```{r}


```

# 13. What is `fit`?

We should demystify what we just did. Execute `fit %>% str(max.level = 1)` to get a sense of the structure of our `fit` object. You'll see it is composed of four upper-level sections. The first two sections, `call` and `call1` provide metadata on the linear model(s) you fit and the imputation method you used in the data. The third section `nmis` simply lists the number of missing values in each of the variables in the imputed data sets. The fourth section `analyses` contains the results from the regression models for to each of the imputed data sets. Since we have four imputed data sets in this example, the `analyses` section is a list of four fit objects. If we had imputed 20 data sets, this would be a list of 20 fit objects instead.

```{r}

```

I know I haven't really asked you to do much, here. Just show the code I told you to execute.

# 14. Summarize the results from your multiple imputation regression model

-  Run the final model and save it as `pooled_fit`

    ```{r}

    ```

-  Now execute `pooled_fit %>% str(max.level = 1)` to get a sense of what your `pooled_fit` object even is.
```{r}

```

> Finally, execute `summary(pooled_fit, conf.int = TRUE)` to get your pooled summary. These are the summary results you'd report in a paper.

```{r}

```

# 15. Listwise deletion

> In the get rid of all the NAs in the `d` dataset

```{r}

```

> Run a lm model (use `lm`). The regression model you are fitting is \`lpo \~ lpr\`. Save the results as \`fit_listwise\`.

```{r}

```

# 16. Mean Imputation

> Now, impute the missing data with the mean for each missing variable (set m = 1). `method = "mean"` will do this. Call this object `mean_imp`

```{r}

```

> Run a lm model (`lpo ~ lpr)` using `with` and store as `mean_missing` . Use the `pool` function to save the lm object.

```{r}


```

# 17. Stochastic Regression Imputation

> Impute the missing data with a regression approach (set m = 1).

```{r}


```

> Run the lm model and store it as `lm_missing` using `with.` Make sure you pool the data (creates an appropriate object for using other functions).

```{r}


```

# 18. Comparison

> Try out the cool `modelsummary` function from `modelsummary(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)` to compare the listwise deletion, mean, and regression imputation objects above. Do you see any differences in the estimates, *SEs*, and $R^2$, *p*-values?

```{r}

```

# The End
